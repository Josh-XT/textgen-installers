services:
  text-generation-webui:
    build:
      context: .
      dockerfile: Dockerfile-text-generation-webui
      args:
        # specify which cuda version your card supports: https://developer.nvidia.com/cuda-gpus
        TORCH_CUDA_ARCH_LIST: ${TORCH_CUDA_ARCH_LIST:-7.5}
        WEBUI_VERSION: ${WEBUI_VERSION:-HEAD}
    environment:
      - OOBABOOGA_FLAGS=${CLI_ARGS}
      - CLI_ARGS=${CLI_ARGS}
    ports:
      - "${HOST_PORT:-7860}:${CONTAINER_PORT:-7860}"
      - "${HOST_API_PORT:-5000}:${CONTAINER_API_PORT:-5000}"
      - "${HOST_API_STREAM_PORT:-5005}:${CONTAINER_API_STREAM_PORT:-5005}"
      - "${HOST_API_OPENAI_PORT:-5001}:${CONTAINER_API_OPENAI_PORT:-5001}"
    stdin_open: true
    tty: true
    volumes:
      - ./models:/app/one-click-installers/text-generation-webui/models/LLMmodels:rw
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]